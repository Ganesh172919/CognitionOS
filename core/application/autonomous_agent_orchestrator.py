"""
Autonomous Agent Orchestration Engine

Advanced orchestration engine for autonomous AI agents with:
- Intelligent task decomposition
- Multi-level planning
- Self-evaluation and iteration
- Context management with memory hierarchy
- Code validation pipeline
- Hallucination detection
- Tool-calling framework with safety boundaries
"""

import logging
import asyncio
from datetime import datetime, timedelta
from typing import Dict, Any, List, Optional, Tuple, Callable
from uuid import UUID, uuid4
from enum import Enum
from dataclasses import dataclass, field

from core.domain.agent.entities import (
    Agent,
    AgentRole,
    AgentStatus,
    AgentId,
    Capability,
    BudgetLimits,
    BudgetUsage,
)
from core.domain.task.entities import Task, TaskStatus
from core.exceptions import (
    AgentException,
    ValidationError,
    ResourceExhaustedException,
)

logger = logging.getLogger(__name__)


class PlanningStrategy(str, Enum):
    """Agent planning strategies."""
    SEQUENTIAL = "sequential"  # One task at a time
    PARALLEL = "parallel"  # Multiple tasks simultaneously
    ADAPTIVE = "adaptive"  # Dynamically adjust based on context
    HIERARCHICAL = "hierarchical"  # Break into sub-plans


class ExecutionMode(str, Enum):
    """Agent execution modes."""
    DETERMINISTIC = "deterministic"  # Reproducible results
    EXPLORATORY = "exploratory"  # Try multiple approaches
    CONSERVATIVE = "conservative"  # Minimize risks
    AGGRESSIVE = "aggressive"  # Maximize speed


class ConfidenceLevel(str, Enum):
    """Agent confidence levels."""
    VERY_HIGH = "very_high"  # > 95%
    HIGH = "high"  # 80-95%
    MEDIUM = "medium"  # 60-80%
    LOW = "low"  # 40-60%
    VERY_LOW = "very_low"  # < 40%


@dataclass
class AgentContext:
    """
    Context for agent execution.
    
    Maintains state across multiple agent interactions.
    """
    agent_id: UUID
    goal: str
    constraints: List[str] = field(default_factory=list)
    memory_entries: List[Dict[str, Any]] = field(default_factory=list)
    tool_results: Dict[str, Any] = field(default_factory=dict)
    iteration_count: int = 0
    confidence_score: float = 0.0
    current_plan: Optional[Dict[str, Any]] = None
    validation_results: List[Dict[str, Any]] = field(default_factory=list)
    
    def update_confidence(self, score: float):
        """Update confidence score with bounds checking."""
        self.confidence_score = max(0.0, min(1.0, score))
    
    def add_memory(self, entry: Dict[str, Any]):
        """Add memory entry with timestamp."""
        entry["timestamp"] = datetime.utcnow().isoformat()
        self.memory_entries.append(entry)
    
    def get_recent_memory(self, count: int = 10) -> List[Dict[str, Any]]:
        """Get most recent memory entries."""
        return self.memory_entries[-count:]


@dataclass
class PlanStep:
    """A single step in an execution plan."""
    step_id: str
    action: str
    reasoning: str
    inputs: Dict[str, Any]
    expected_output: str
    validation_criteria: List[str]
    dependencies: List[str] = field(default_factory=list)
    confidence: float = 0.0
    estimated_cost: float = 0.0
    estimated_duration: float = 0.0
    status: str = "pending"
    result: Optional[Dict[str, Any]] = None


@dataclass
class ExecutionPlan:
    """
    Multi-step execution plan generated by planning agent.
    """
    plan_id: str
    goal: str
    steps: List[PlanStep]
    strategy: PlanningStrategy
    created_at: datetime
    overall_confidence: float
    estimated_total_cost: float
    estimated_total_duration: float
    risk_factors: List[str] = field(default_factory=list)
    
    def get_next_steps(self) -> List[PlanStep]:
        """Get steps ready for execution (dependencies met)."""
        completed_steps = {s.step_id for s in self.steps if s.status == "completed"}
        
        return [
            step for step in self.steps
            if step.status == "pending"
            and all(dep in completed_steps for dep in step.dependencies)
        ]
    
    def mark_step_completed(self, step_id: str, result: Dict[str, Any]):
        """Mark a step as completed with result."""
        for step in self.steps:
            if step.step_id == step_id:
                step.status = "completed"
                step.result = result
                break


class AutonomousAgentOrchestrator:
    """
    Advanced autonomous agent orchestration engine.
    
    Coordinates multiple specialized agents to accomplish complex goals
    with self-evaluation, iteration, and intelligent planning.
    """
    
    def __init__(
        self,
        planning_agent: Any,
        execution_agent: Any,
        validation_agent: Any,
        memory_service: Any,
        tool_registry: Any,
    ):
        """
        Initialize orchestrator with specialized agents.
        
        Args:
            planning_agent: Agent for planning and decomposition
            execution_agent: Agent for executing tasks
            validation_agent: Agent for validating results
            memory_service: Long-term memory service
            tool_registry: Registry of available tools
        """
        self.planning_agent = planning_agent
        self.execution_agent = execution_agent
        self.validation_agent = validation_agent
        self.memory_service = memory_service
        self.tool_registry = tool_registry
        
        self.active_contexts: Dict[UUID, AgentContext] = {}
        self.execution_history: List[Dict[str, Any]] = []
        
        logger.info("Autonomous agent orchestrator initialized")
    
    async def execute_goal(
        self,
        goal: str,
        constraints: Optional[List[str]] = None,
        mode: ExecutionMode = ExecutionMode.DETERMINISTIC,
        max_iterations: int = 5,
        budget_limits: Optional[BudgetLimits] = None,
    ) -> Dict[str, Any]:
        """
        Execute a high-level goal autonomously.
        
        Orchestrates the full lifecycle:
        1. Create execution context
        2. Generate execution plan
        3. Execute plan steps
        4. Validate results
        5. Iterate if needed
        6. Return final result
        
        Args:
            goal: High-level goal to accomplish
            constraints: Optional constraints and requirements
            mode: Execution mode
            max_iterations: Maximum number of iteration cycles
            budget_limits: Resource budget limits
            
        Returns:
            Execution result with status, outputs, and metadata
        """
        agent_id = uuid4()
        context = AgentContext(
            agent_id=agent_id,
            goal=goal,
            constraints=constraints or [],
        )
        self.active_contexts[agent_id] = context
        
        logger.info(f"Starting autonomous execution for goal: {goal}")
        
        budget = BudgetUsage()
        start_time = datetime.utcnow()
        
        try:
            # Iteration loop
            for iteration in range(max_iterations):
                context.iteration_count = iteration + 1
                logger.info(f"Iteration {iteration + 1}/{max_iterations}")
                
                # Step 1: Planning
                plan = await self._generate_plan(context, mode)
                context.current_plan = plan
                
                # Step 2: Execution
                execution_result = await self._execute_plan(plan, context, budget, budget_limits)
                
                # Step 3: Validation
                validation_result = await self._validate_result(
                    execution_result,
                    context,
                )
                context.validation_results.append(validation_result)
                
                # Step 4: Self-evaluation
                should_continue, confidence = await self._evaluate_result(
                    execution_result,
                    validation_result,
                    context,
                )
                context.update_confidence(confidence)
                
                # Check if we're done
                if should_continue == False or confidence >= 0.9:
                    logger.info(f"Goal achieved with confidence {confidence:.2f}")
                    break
                
                # Update context for next iteration
                context.add_memory({
                    "iteration": iteration + 1,
                    "plan": plan,
                    "result": execution_result,
                    "validation": validation_result,
                    "confidence": confidence,
                })
                
                # Check budget
                if budget_limits:
                    if budget.cost_usd >= budget_limits.max_cost_usd:
                        raise ResourceExhaustedException("Budget exceeded")
            
            # Final result
            duration = (datetime.utcnow() - start_time).total_seconds()
            
            return {
                "status": "success",
                "goal": goal,
                "result": execution_result,
                "confidence": context.confidence_score,
                "iterations": context.iteration_count,
                "duration_seconds": duration,
                "budget_used": {
                    "tokens": budget.tokens_used,
                    "cost_usd": budget.cost_usd,
                },
                "plan_summary": self._summarize_plan(plan),
            }
            
        except Exception as e:
            logger.error(f"Autonomous execution failed: {e}", exc_info=True)
            return {
                "status": "failed",
                "goal": goal,
                "error": str(e),
                "iterations": context.iteration_count,
                "partial_result": context.tool_results,
            }
        finally:
            # Cleanup context
            if agent_id in self.active_contexts:
                del self.active_contexts[agent_id]
    
    async def _generate_plan(
        self,
        context: AgentContext,
        mode: ExecutionMode,
    ) -> ExecutionPlan:
        """
        Generate execution plan using planning agent.
        
        Uses intelligent task decomposition with multi-level planning.
        """
        logger.info("Generating execution plan")
        
        # Retrieve relevant memories
        relevant_memories = await self.memory_service.retrieve(
            user_id=str(context.agent_id),
            query=context.goal,
            k=5,
        )
        
        # Build planning prompt
        prompt = self._build_planning_prompt(
            goal=context.goal,
            constraints=context.constraints,
            memories=relevant_memories,
            mode=mode,
            previous_attempts=context.validation_results,
        )
        
        # Call planning agent
        planning_result = await self.planning_agent.plan(
            prompt=prompt,
            context=context,
        )
        
        # Parse and validate plan
        plan = self._parse_plan(planning_result)
        
        # Estimate costs and durations
        self._estimate_plan_costs(plan)
        
        # Identify risks
        plan.risk_factors = self._identify_risks(plan)
        
        logger.info(f"Generated plan with {len(plan.steps)} steps, "
                   f"confidence: {plan.overall_confidence:.2f}")
        
        return plan
    
    async def _execute_plan(
        self,
        plan: ExecutionPlan,
        context: AgentContext,
        budget: BudgetUsage,
        budget_limits: Optional[BudgetLimits],
    ) -> Dict[str, Any]:
        """
        Execute plan steps using execution agent.
        
        Supports parallel execution for independent steps.
        """
        logger.info("Executing plan")
        
        results = {}
        
        while True:
            # Get next steps ready for execution
            next_steps = plan.get_next_steps()
            
            if not next_steps:
                break  # All steps complete
            
            # Execute steps (parallel if possible)
            if plan.strategy == PlanningStrategy.PARALLEL and len(next_steps) > 1:
                step_results = await asyncio.gather(
                    *[self._execute_step(step, context, budget, budget_limits) 
                      for step in next_steps],
                    return_exceptions=True,
                )
            else:
                # Sequential execution
                step_results = []
                for step in next_steps:
                    result = await self._execute_step(step, context, budget, budget_limits)
                    step_results.append(result)
            
            # Update plan with results
            for step, result in zip(next_steps, step_results):
                if isinstance(result, Exception):
                    logger.error(f"Step {step.step_id} failed: {result}")
                    step.status = "failed"
                else:
                    plan.mark_step_completed(step.step_id, result)
                    results[step.step_id] = result
        
        return {
            "plan_id": plan.plan_id,
            "steps_completed": len([s for s in plan.steps if s.status == "completed"]),
            "total_steps": len(plan.steps),
            "results": results,
        }
    
    async def _execute_step(
        self,
        step: PlanStep,
        context: AgentContext,
        budget: BudgetUsage,
        budget_limits: Optional[BudgetLimits],
    ) -> Dict[str, Any]:
        """Execute a single plan step with safety checks."""
        logger.info(f"Executing step: {step.step_id} - {step.action}")
        
        # Check budget before execution
        if budget_limits:
            if budget.cost_usd >= budget_limits.max_cost_usd:
                raise ResourceExhaustedException("Budget limit reached")
        
        # Build execution prompt
        prompt = self._build_execution_prompt(step, context)
        
        # Execute with timeout
        start_time = datetime.utcnow()
        
        try:
            result = await asyncio.wait_for(
                self.execution_agent.execute(
                    prompt=prompt,
                    tools=self._get_available_tools(step),
                    context=context,
                ),
                timeout=step.estimated_duration * 2  # 2x estimated time
            )
            
            # Update budget
            execution_time = (datetime.utcnow() - start_time).total_seconds()
            budget.time_seconds += execution_time
            budget.cost_usd += result.get("cost", 0.0)
            budget.tokens_used += result.get("tokens", 0)
            
            # Store in context
            context.tool_results[step.step_id] = result
            
            return result
            
        except asyncio.TimeoutError:
            logger.error(f"Step {step.step_id} timed out")
            raise AgentException(f"Step execution timed out: {step.step_id}")
    
    async def _validate_result(
        self,
        result: Dict[str, Any],
        context: AgentContext,
    ) -> Dict[str, Any]:
        """
        Validate execution results using validation agent.
        
        Checks:
        - Output correctness
        - Constraint satisfaction
        - Quality metrics
        - Hallucination detection
        """
        logger.info("Validating execution result")
        
        validation_prompt = self._build_validation_prompt(
            goal=context.goal,
            result=result,
            constraints=context.constraints,
        )
        
        validation_result = await self.validation_agent.validate(
            prompt=validation_prompt,
            context=context,
        )
        
        # Check for hallucinations
        hallucination_check = await self._check_hallucinations(
            result=result,
            context=context,
        )
        validation_result["hallucination_detected"] = hallucination_check
        
        return validation_result
    
    async def _evaluate_result(
        self,
        execution_result: Dict[str, Any],
        validation_result: Dict[str, Any],
        context: AgentContext,
    ) -> Tuple[bool, float]:
        """
        Self-evaluate result quality and determine if iteration is needed.
        
        Returns:
            Tuple of (should_continue_iteration, confidence_score)
        """
        # Calculate confidence based on validation
        confidence = validation_result.get("confidence", 0.5)
        
        # Check if hallucination detected
        if validation_result.get("hallucination_detected", False):
            confidence *= 0.5  # Reduce confidence
        
        # Check constraint satisfaction
        constraints_met = validation_result.get("constraints_met", 0)
        total_constraints = len(context.constraints)
        if total_constraints > 0:
            constraint_score = constraints_met / total_constraints
            confidence *= constraint_score
        
        # Determine if we should continue
        should_continue = confidence < 0.9 and context.iteration_count < 5
        
        logger.info(f"Self-evaluation: confidence={confidence:.2f}, continue={should_continue}")
        
        return should_continue, confidence
    
    async def _check_hallucinations(
        self,
        result: Dict[str, Any],
        context: AgentContext,
    ) -> bool:
        """
        Check for hallucinations in agent output.
        
        Looks for:
        - Fabricated information
        - Contradictions
        - Unsupported claims
        """
        # Implementation would use validation techniques
        # For now, basic check
        return False
    
    def _build_planning_prompt(
        self,
        goal: str,
        constraints: List[str],
        memories: List[Dict[str, Any]],
        mode: ExecutionMode,
        previous_attempts: List[Dict[str, Any]],
    ) -> str:
        """Build prompt for planning agent."""
        prompt = f"""Goal: {goal}

Mode: {mode}

Constraints:
{chr(10).join(f"- {c}" for c in constraints)}

"""
        
        if memories:
            prompt += "\nRelevant Context:\n"
            for mem in memories:
                prompt += f"- {mem.get('content', '')}\n"
        
        if previous_attempts:
            prompt += f"\nPrevious Attempts: {len(previous_attempts)}\n"
            for i, attempt in enumerate(previous_attempts, 1):
                prompt += f"Attempt {i}: confidence={attempt.get('confidence', 0):.2f}\n"
        
        prompt += "\nGenerate a detailed execution plan with steps, reasoning, and validation criteria."
        
        return prompt
    
    def _build_execution_prompt(
        self,
        step: PlanStep,
        context: AgentContext,
    ) -> str:
        """Build prompt for execution agent."""
        return f"""Execute: {step.action}

Reasoning: {step.reasoning}

Inputs: {step.inputs}

Expected Output: {step.expected_output}

Previous Results: {list(context.tool_results.keys())}
"""
    
    def _build_validation_prompt(
        self,
        goal: str,
        result: Dict[str, Any],
        constraints: List[str],
    ) -> str:
        """Build prompt for validation agent."""
        return f"""Validate if this result achieves the goal:

Goal: {goal}

Result: {result}

Constraints to check:
{chr(10).join(f"- {c}" for c in constraints)}

Provide confidence score and identify any issues.
"""
    
    def _parse_plan(self, planning_result: Dict[str, Any]) -> ExecutionPlan:
        """Parse planning agent output into ExecutionPlan."""
        steps = []
        for i, step_data in enumerate(planning_result.get("steps", []), 1):
            steps.append(PlanStep(
                step_id=f"step_{i}",
                action=step_data.get("action", ""),
                reasoning=step_data.get("reasoning", ""),
                inputs=step_data.get("inputs", {}),
                expected_output=step_data.get("expected_output", ""),
                validation_criteria=step_data.get("validation_criteria", []),
                dependencies=step_data.get("dependencies", []),
                confidence=step_data.get("confidence", 0.5),
            ))
        
        return ExecutionPlan(
            plan_id=str(uuid4()),
            goal=planning_result.get("goal", ""),
            steps=steps,
            strategy=PlanningStrategy(planning_result.get("strategy", "sequential")),
            created_at=datetime.utcnow(),
            overall_confidence=planning_result.get("confidence", 0.5),
            estimated_total_cost=0.0,
            estimated_total_duration=0.0,
        )
    
    def _estimate_plan_costs(self, plan: ExecutionPlan):
        """Estimate costs and durations for plan steps."""
        total_cost = 0.0
        total_duration = 0.0
        
        for step in plan.steps:
            # Basic estimation (would be more sophisticated in production)
            step.estimated_cost = 0.01  # $0.01 per step
            step.estimated_duration = 10.0  # 10 seconds per step
            
            total_cost += step.estimated_cost
            total_duration += step.estimated_duration
        
        plan.estimated_total_cost = total_cost
        plan.estimated_total_duration = total_duration
    
    def _identify_risks(self, plan: ExecutionPlan) -> List[str]:
        """Identify potential risks in execution plan."""
        risks = []
        
        if plan.overall_confidence < 0.6:
            risks.append("Low overall confidence")
        
        if len(plan.steps) > 10:
            risks.append("Complex plan with many steps")
        
        if plan.estimated_total_cost > 0.5:
            risks.append("High estimated cost")
        
        return risks
    
    def _get_available_tools(self, step: PlanStep) -> List[str]:
        """Get list of tools available for step execution."""
        # Would integrate with tool registry
        return ["code_execution", "web_search", "file_operations"]
    
    def _summarize_plan(self, plan: ExecutionPlan) -> Dict[str, Any]:
        """Create summary of execution plan."""
        return {
            "total_steps": len(plan.steps),
            "steps_completed": len([s for s in plan.steps if s.status == "completed"]),
            "strategy": plan.strategy,
            "confidence": plan.overall_confidence,
        }
